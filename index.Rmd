```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=6, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE)
```

---
title: "Exercise prediction"
author: "jn"
date: "December 27, 2015"
output: html_document
---

###Data read-in###
```{r }
setwd("C:/Users/Julius/OneDrive/Documents/Studies/Practical Machine Learning/project")
df<-read.csv('pml-training.csv', header = TRUE, stringsAsFactors = FALSE)
```

Supplied sample data set has `r dim(df)[1]` observations and `r dim(df)[2]` variables. All data is read with option `stringAsFactors=FALSE`. It is known that variable that will need to predicted is categorical, therefore it is converted to factor variable: 
```{r}
df$classe<-as.factor(df$classe)
str(df$classe)
```


###Data slicing###

```{r echo=TRUE}
library(caret)
set.seed(25268)
inTrain<-createDataPartition(y=df$classe, p=0.6, list=FALSE)
training<-df[inTrain,]; testing<-df[-inTrain,]
```
For cross-validation purposes 40% of sample is withheld. After training sample slicing, training set now contains `r dim(training)[1]` observations and on sample test set has `r dim(testing)[1]`.

###Data cleaning###

In training data set there are 67 variables that have more than 50% of observations missing. They are all removed from training set. 
```{r echo=TRUE}
NAs<-lapply(training,function(x){sum(is.na(x))})
training<-training[,which(NAs==0)]
```

##Feature selection## 

Data comes from Weight Lifting Data set used in [project](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) predicting how well exercise activities were performed by test subjects. Authors used experiment specific features that are relevant to experiment setting and will not be used to train data. Those features were calculated from continuous time measurements of sensors and are following `kurtosis_`,`skewness_`,`min_`,`max_`,`amplitude_`. Additionally 7 more features are excluded: `x` - index of data, `user_name` of test subject and 5 more variables used in calculating sensor features that were removed from training set. Here is final list of `52` features that are used in training: 
```{r}
training<-training[, grep("^(kurtosis_)", names(training), value = TRUE, invert = TRUE)]
training<-training[, grep("^(skewness_)", names(training), value = TRUE, invert = TRUE)]
training<-training[, grep("^(max_)", names(training), value = TRUE, invert = TRUE)]
training<-training[, grep("^(min_)", names(training), value = TRUE, invert = TRUE)]
training<-training[, grep("^(amplitude_)", names(training), value = TRUE, invert = TRUE)]
training<-training[,-(1:7)]
names(training[,-53])

#t(cor(as.matrix(as.numeric(training[,53])), as.matrix(training[,-53])))
```
As it is classification problem, no additional data preprocessing is done, as it is unlikely to give tangible extra benefit. This might be reconsidered if in sample error rate of trained model will be of unacceptable levels.  


###Model training###

For training model R package `caret` and its `train()` function is used. Method chosen is random forests `rf`.  Training control of K-fold cross validation with 5 folds was used and `mtry` set to default value. After running training, following model is automatically selected by `train()`. 

```{r echo=TRUE}
set.seed(1000)
fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)
rndFit<-train(x=training[,-53],
           y=training$classe,
           method = "rf",
           trControl = fitControl)
rndFit$finalModel

```
Out of sample error rate as suggested by OOB estimate of error rate is 0.91%.  
Further in sample prediction test is done.  
Below is confusion matrix on withheld in-sample test set prediction.
```{r echo=TRUE}
testing<-testing[,names(training)]
confusionMatrix(predict(rndFit,testing[,-53]), testing$classe)
```
In sample test model performed with 99.2% accuracy. This is sufficient accuracy. 

###Prediction###

Prediction is done on out of sample testing set. And results are following. 
```{r}
validation<-read.csv('pml-testing.csv', header = TRUE, stringsAsFactors = FALSE)
testResults<-predict(rndFit,validation[names(training[,-53])])
testResults
```
These results where submitted and prediction had 100% success rate on 20 cases.
